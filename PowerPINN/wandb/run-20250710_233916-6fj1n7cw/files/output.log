GFL_2nd_order data
Loading data from:  ./data/GFL_2nd_order/dataset_v1.pkl
/Users/nbhsbgnb/PycharmProjects/PythonProject/PowerPINN/src/nn/nn_dataset.py:178: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)
  training_sample = torch.tensor(training_sample, dtype=torch.float32) # convert the trajectory to tensor
Number of training samples:  80000 Number of validation samples:  10000 Number of testing samples:  10000
Number of different initial conditions for collocation points:  100
['delta', 'omega'] Variables
[[-3.14159, 3.14159], [-60, 60]] Set of values for init conditions
[10, 10] Iterations per value
Shape: (100, 2)
Selected deep learning model:  DynamicNN
Number of labeled training data: 3479 Number of collocation points: 5264 Number of collocation points (IC): 100 Number of validation data: 2500
Weights initialized as:  [1, 0.001, 0.0001, 0.001]  are updated with scheme:  Static
getting in training
Validation loss decreased (inf --> 143.775650).  Saving model ...
Epoch [50/15000], Loss: 183.8573, Loss_data: 118.0710, Loss_dt: 65134.4570, Loss_pinn: 598.7888 , Loss_pinn_ic : 591.8987 139.18533325195312 78046.75
Epoch [100/15000], Loss: 183.4502, Loss_data: 117.6706, Loss_dt: 65134.4336, Loss_pinn: 540.3666 , Loss_pinn_ic : 591.1202 139.2402801513672 78046.5078125
Epoch [150/15000], Loss: 183.0698, Loss_data: 117.2897, Loss_dt: 65133.7578, Loss_pinn: 559.7413 , Loss_pinn_ic : 590.3785 139.34295654296875 78045.0859375
Epoch [200/15000], Loss: 182.1251, Loss_data: 116.3397, Loss_dt: 65128.4375, Loss_pinn: 716.4562 , Loss_pinn_ic : 585.3293 138.65216064453125 78036.1484375
Epoch [250/15000], Loss: 180.0759, Loss_data: 114.3269, Loss_dt: 65100.1055, Loss_pinn: 837.2006 , Loss_pinn_ic : 565.1247 137.9601593017578 77993.75
Epoch [300/15000], Loss: 175.0968, Loss_data: 109.5628, Loss_dt: 64945.7227, Loss_pinn: 941.3021 , Loss_pinn_ic : 494.1684 135.22634887695312 77811.6875
Epoch [350/15000], Loss: 165.6899, Loss_data: 100.8201, Loss_dt: 64408.7383, Loss_pinn: 868.1424 , Loss_pinn_ic : 374.2149 128.67967224121094 77058.7578125
Epoch [400/15000], Loss: 150.9583, Loss_data: 88.4835, Loss_dt: 62118.2812, Loss_pinn: 870.3815 , Loss_pinn_ic : 269.5081 120.42852783203125 75009.2734375
Epoch [450/15000], Loss: 132.3801, Loss_data: 74.8399, Loss_dt: 57240.2656, Loss_pinn: 1178.8334 , Loss_pinn_ic : 182.0889 106.14131164550781 71320.4609375
Epoch [500/15000], Loss: 115.1105, Loss_data: 63.3450, Loss_dt: 51429.1328, Loss_pinn: 1888.7218 , Loss_pinn_ic : 147.5210 100.6684799194336 69830.3203125
Epoch [550/15000], Loss: 99.9852, Loss_data: 54.1541, Loss_dt: 45419.8359, Loss_pinn: 2933.1792 , Loss_pinn_ic : 117.9462 97.87432861328125 68383.46875
Epoch [600/15000], Loss: 89.2239, Loss_data: 47.7524, Loss_dt: 41154.4375, Loss_pinn: 2141.3879 , Loss_pinn_ic : 102.9175 97.45481872558594 67470.640625
Epoch [650/15000], Loss: 81.7386, Loss_data: 44.0872, Loss_dt: 37377.4961, Loss_pinn: 1817.9419 , Loss_pinn_ic : 92.1326 97.58599090576172 67662.109375
Epoch [700/15000], Loss: 75.2174, Loss_data: 40.4629, Loss_dt: 34457.3086, Loss_pinn: 2320.1792 , Loss_pinn_ic : 65.1514 95.3563461303711 65309.578125
Epoch [750/15000], Loss: 69.8234, Loss_data: 37.7594, Loss_dt: 31711.9336, Loss_pinn: 2966.4590 , Loss_pinn_ic : 55.4513 96.74618530273438 66431.8359375
Epoch [800/15000], Loss: 64.8529, Loss_data: 35.2475, Loss_dt: 29188.0352, Loss_pinn: 3587.1667 , Loss_pinn_ic : 58.6239 98.35297393798828 67981.15625
Epoch [850/15000], Loss: 59.9846, Loss_data: 32.6329, Loss_dt: 26898.8828, Loss_pinn: 3942.8748 , Loss_pinn_ic : 58.4704 98.32217407226562 67595.484375
Epoch [900/15000], Loss: 56.0201, Loss_data: 30.4934, Loss_dt: 25048.5996, Loss_pinn: 4164.7754 , Loss_pinn_ic : 61.6673 99.18045043945312 68059.3515625
Epoch [950/15000], Loss: 52.3261, Loss_data: 28.5089, Loss_dt: 23327.5449, Loss_pinn: 4285.4082 , Loss_pinn_ic : 61.1771 98.08382415771484 65907.703125
Epoch [1000/15000], Loss: 49.0090, Loss_data: 26.7699, Loss_dt: 21729.2480, Loss_pinn: 4443.4395 , Loss_pinn_ic : 65.4272 98.99932861328125 66382.453125
Epoch [1050/15000], Loss: 46.4200, Loss_data: 25.3108, Loss_dt: 20595.8809, Loss_pinn: 4509.5337 , Loss_pinn_ic : 62.4109 99.19757843017578 65720.765625
Epoch [1100/15000], Loss: 44.0973, Loss_data: 24.0633, Loss_dt: 19513.2012, Loss_pinn: 4608.6299 , Loss_pinn_ic : 59.9616 100.72406005859375 66315.3828125
Epoch [1150/15000], Loss: 42.4270, Loss_data: 23.2006, Loss_dt: 18700.5078, Loss_pinn: 4687.7554 , Loss_pinn_ic : 57.1290 102.71675872802734 67227.9921875
Epoch [1200/15000], Loss: 40.2692, Loss_data: 22.0489, Loss_dt: 17688.2207, Loss_pinn: 4791.1538 , Loss_pinn_ic : 52.9267 103.73786163330078 67539.2734375
Epoch [1250/15000], Loss: 38.5590, Loss_data: 21.1915, Loss_dt: 16822.9199, Loss_pinn: 4942.3994 , Loss_pinn_ic : 50.3438 105.75027465820312 68738.28125
Epoch [1300/15000], Loss: 36.7926, Loss_data: 20.3366, Loss_dt: 15897.1182, Loss_pinn: 5096.1538 , Loss_pinn_ic : 49.2519 107.24054718017578 69661.28125
Epoch [1350/15000], Loss: 35.0687, Loss_data: 19.3709, Loss_dt: 15132.1641, Loss_pinn: 5188.6177 , Loss_pinn_ic : 46.8598 107.29168701171875 69038.109375
Epoch [1400/15000], Loss: 33.8417, Loss_data: 18.7467, Loss_dt: 14519.9668, Loss_pinn: 5260.7783 , Loss_pinn_ic : 48.9568 107.7603759765625 69322.4375
Epoch [1450/15000], Loss: 32.6853, Loss_data: 18.1853, Loss_dt: 13919.7705, Loss_pinn: 5320.1465 , Loss_pinn_ic : 48.2171 111.81273651123047 72248.015625
Epoch [1500/15000], Loss: 30.7045, Loss_data: 17.2768, Loss_dt: 12834.0791, Loss_pinn: 5435.0444 , Loss_pinn_ic : 50.2054 110.66911315917969 71539.328125
Epoch [1550/15000], Loss: 30.9327, Loss_data: 17.2668, Loss_dt: 13056.8115, Loss_pinn: 5549.8936 , Loss_pinn_ic : 54.0658 109.79837799072266 70977.7109375
Epoch [1600/15000], Loss: 29.1063, Loss_data: 16.4238, Loss_dt: 12069.1367, Loss_pinn: 5636.9707 , Loss_pinn_ic : 49.6165 114.736328125 74229.6328125
Epoch [1650/15000], Loss: 27.2952, Loss_data: 15.6589, Loss_dt: 11020.2832, Loss_pinn: 5659.6904 , Loss_pinn_ic : 50.0507 113.47329711914062 73706.7578125
Epoch [1700/15000], Loss: 26.8204, Loss_data: 15.5542, Loss_dt: 10653.2178, Loss_pinn: 5609.4126 , Loss_pinn_ic : 52.0536 113.59956359863281 74323.828125
Epoch [1750/15000], Loss: 26.1292, Loss_data: 15.0879, Loss_dt: 10442.6348, Loss_pinn: 5506.9468 , Loss_pinn_ic : 48.0415 114.29541015625 73795.2109375
Epoch [1800/15000], Loss: 24.9377, Loss_data: 14.6042, Loss_dt: 9740.5625, Loss_pinn: 5468.4146 , Loss_pinn_ic : 46.0636 116.67158508300781 76141.8359375
Epoch [1850/15000], Loss: 24.5333, Loss_data: 14.3949, Loss_dt: 9558.0186, Loss_pinn: 5361.5781 , Loss_pinn_ic : 44.2425 115.90789031982422 74436.59375
Epoch [1900/15000], Loss: 23.5446, Loss_data: 14.0158, Loss_dt: 8956.8115, Loss_pinn: 5294.2671 , Loss_pinn_ic : 42.5464 118.06536102294922 75662.8046875
Epoch [1950/15000], Loss: 23.2564, Loss_data: 13.8425, Loss_dt: 8855.0352, Loss_pinn: 5183.9233 , Loss_pinn_ic : 40.4843 119.33045196533203 75706.46875
Epoch [2000/15000], Loss: 22.5569, Loss_data: 13.5550, Loss_dt: 8452.4814, Loss_pinn: 5106.5083 , Loss_pinn_ic : 38.7855 122.44811248779297 78234.3671875
Epoch [2050/15000], Loss: 21.7400, Loss_data: 13.2176, Loss_dt: 7982.8237, Loss_pinn: 5022.4438 , Loss_pinn_ic : 37.3245 122.0467529296875 76889.8359375
Epoch [2100/15000], Loss: 21.7510, Loss_data: 13.1859, Loss_dt: 8034.9395, Loss_pinn: 4931.6743 , Loss_pinn_ic : 37.0049 125.90547180175781 80400.328125
Epoch [2150/15000], Loss: 20.7932, Loss_data: 12.7984, Loss_dt: 7476.1499, Loss_pinn: 4831.2993 , Loss_pinn_ic : 35.5208 124.46064758300781 77708.40625
Epoch [2200/15000], Loss: 19.8370, Loss_data: 12.4176, Loss_dt: 6900.1846, Loss_pinn: 4849.0098 , Loss_pinn_ic : 34.2929 126.49256134033203 78771.7265625
Epoch [2250/15000], Loss: 19.2409, Loss_data: 12.1961, Loss_dt: 6532.0854, Loss_pinn: 4791.5552 , Loss_pinn_ic : 33.6278 127.11505126953125 78294.5
Epoch [2300/15000], Loss: 19.3507, Loss_data: 12.1668, Loss_dt: 6677.0122, Loss_pinn: 4734.0371 , Loss_pinn_ic : 33.4125 127.5323257446289 76933.4765625
Epoch [2350/15000], Loss: 18.0515, Loss_data: 11.7559, Loss_dt: 5784.4033, Loss_pinn: 4789.7793 , Loss_pinn_ic : 32.2478 129.92730712890625 78338.46875
Epoch [2400/15000], Loss: 17.7487, Loss_data: 11.6916, Loss_dt: 5544.3232, Loss_pinn: 4804.8540 , Loss_pinn_ic : 32.3663 132.16671752929688 80545.8515625
Epoch [2450/15000], Loss: 17.2105, Loss_data: 11.5700, Loss_dt: 5128.2715, Loss_pinn: 4804.0293 , Loss_pinn_ic : 31.8471 132.56654357910156 80807.8828125
Epoch [2500/15000], Loss: 16.4268, Loss_data: 11.2553, Loss_dt: 4653.6255, Loss_pinn: 4873.3340 , Loss_pinn_ic : 30.5710 133.14051818847656 79870.796875
Epoch [2550/15000], Loss: 15.9122, Loss_data: 11.0940, Loss_dt: 4301.5698, Loss_pinn: 4858.0356 , Loss_pinn_ic : 30.7565 133.37100219726562 80680.1953125
Epoch [2600/15000], Loss: 15.6220, Loss_data: 10.9926, Loss_dt: 4112.8359, Loss_pinn: 4859.9766 , Loss_pinn_ic : 30.6151 135.63754272460938 82297.1171875
Epoch [2650/15000], Loss: 15.3603, Loss_data: 10.9269, Loss_dt: 3928.9822, Loss_pinn: 4744.4473 , Loss_pinn_ic : 29.9128 137.10812377929688 83656.015625
Epoch [2700/15000], Loss: 14.4977, Loss_data: 10.5592, Loss_dt: 3428.5037, Loss_pinn: 4820.9541 , Loss_pinn_ic : 27.9219 137.81019592285156 83609.1171875
Epoch [2750/15000], Loss: 13.9786, Loss_data: 10.3819, Loss_dt: 3092.8323, Loss_pinn: 4763.5034 , Loss_pinn_ic : 27.4898 139.2837677001953 84609.3984375
Epoch [2800/15000], Loss: 14.5738, Loss_data: 10.6019, Loss_dt: 3455.1155, Loss_pinn: 4885.1450 , Loss_pinn_ic : 28.2785 139.9207000732422 86319.1875
Epoch [2850/15000], Loss: 13.0768, Loss_data: 10.0576, Loss_dt: 2513.5530, Loss_pinn: 4796.0771 , Loss_pinn_ic : 25.9682 142.12704467773438 86897.71875
Epoch [2900/15000], Loss: 12.6710, Loss_data: 9.9197, Loss_dt: 2249.0378, Loss_pinn: 4767.8813 , Loss_pinn_ic : 25.4885 143.88796997070312 87946.2890625
Epoch [2950/15000], Loss: 12.3328, Loss_data: 9.8167, Loss_dt: 2013.7842, Loss_pinn: 4768.4014 , Loss_pinn_ic : 25.5125 145.34909057617188 89117.6171875
Epoch [3000/15000], Loss: 12.0367, Loss_data: 9.7085, Loss_dt: 1831.7709, Loss_pinn: 4713.0835 , Loss_pinn_ic : 25.1334 147.83409118652344 90439.46875
Epoch [3050/15000], Loss: 12.1575, Loss_data: 9.7860, Loss_dt: 1887.6107, Loss_pinn: 4597.3589 , Loss_pinn_ic : 24.1535 147.8856201171875 89718.5625
Epoch [3100/15000], Loss: 11.3613, Loss_data: 9.4494, Loss_dt: 1434.9570, Loss_pinn: 4531.8867 , Loss_pinn_ic : 23.7415 150.55160522460938 91704.9609375
Epoch [3150/15000], Loss: 11.1340, Loss_data: 9.3755, Loss_dt: 1293.5474, Loss_pinn: 4417.3784 , Loss_pinn_ic : 23.1849 152.2516632080078 92682.6796875
Early stopping
Validation loss decreased (143.775650 --> 153.192947).  Saving model ...
Model( and tf values) saved: model/data_dt_pinn_ic/GFL_2nd_orderDynamicNN_1_3181_3479_5264_2500_None_None_1_0.001_0.0001_0.001_Static.pth
Total test trajectories 10
Loss: 114.52862549
MAE Loss: 3.95198631
Total trainable parameters 17026
