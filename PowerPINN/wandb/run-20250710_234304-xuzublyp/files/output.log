GFL_2nd_order data
Loading data from:  ./data/GFL_2nd_order/dataset_v1.pkl
/Users/nbhsbgnb/PycharmProjects/PythonProject/PowerPINN/src/nn/nn_dataset.py:178: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)
  training_sample = torch.tensor(training_sample, dtype=torch.float32) # convert the trajectory to tensor
Number of training samples:  80000 Number of validation samples:  10000 Number of testing samples:  10000
Number of different initial conditions for collocation points:  100
['delta', 'omega'] Variables
[[-3.14159, 3.14159], [-60, 60]] Set of values for init conditions
[10, 10] Iterations per value
Shape: (100, 2)
Selected deep learning model:  DynamicNN
Number of labeled training data: 80000 Number of collocation points: 100000 Number of collocation points (IC): 100 Number of validation data: 10000
Weights initialized as:  [1, 0.001, 0.0001, 0.001]  are updated with scheme:  Static
getting in training
Validation loss decreased (inf --> 142.703156).  Saving model ...
Epoch [50/15000], Loss: 183.4436, Loss_data: 117.9615, Loss_dt: 64830.0586, Loss_pinn: 599.7886 , Loss_pinn_ic : 592.0711 138.2398223876953 77586.0390625
Epoch [100/15000], Loss: 183.0347, Loss_data: 117.5591, Loss_dt: 64830.0039, Loss_pinn: 542.2007 , Loss_pinn_ic : 591.3095 138.29351806640625 77585.78125
Epoch [150/15000], Loss: 182.6579, Loss_data: 117.1814, Loss_dt: 64829.1836, Loss_pinn: 568.6058 , Loss_pinn_ic : 590.4812 138.39614868164062 77584.3671875
Epoch [200/15000], Loss: 181.7246, Loss_data: 116.2441, Loss_dt: 64823.5312, Loss_pinn: 713.8096 , Loss_pinn_ic : 585.6101 137.73934936523438 77575.7734375
Epoch [250/15000], Loss: 179.6802, Loss_data: 114.2370, Loss_dt: 64794.1445, Loss_pinn: 840.9137 , Loss_pinn_ic : 565.0047 137.0653076171875 77534.6328125
Epoch [300/15000], Loss: 174.6810, Loss_data: 109.4530, Loss_dt: 64632.7695, Loss_pinn: 962.3240 , Loss_pinn_ic : 499.0692 134.35443115234375 77353.546875
Epoch [350/15000], Loss: 165.2621, Loss_data: 100.7134, Loss_dt: 64089.1328, Loss_pinn: 885.5104 , Loss_pinn_ic : 371.0437 127.65640258789062 76614.2890625
Epoch [400/15000], Loss: 150.2576, Loss_data: 88.2135, Loss_dt: 61688.1328, Loss_pinn: 948.4794 , Loss_pinn_ic : 261.0212 119.78810119628906 74424.875
Epoch [450/15000], Loss: 132.1119, Loss_data: 74.6477, Loss_dt: 57159.1523, Loss_pinn: 1104.1195 , Loss_pinn_ic : 194.6303 109.00364685058594 73150.0546875
Epoch [500/15000], Loss: 113.8393, Loss_data: 62.7482, Loss_dt: 50757.2383, Loss_pinn: 2011.2500 , Loss_pinn_ic : 132.7788 98.35240173339844 67613.8125
Epoch [550/15000], Loss: 99.1014, Loss_data: 53.6095, Loss_dt: 45072.5391, Loss_pinn: 3040.5923 , Loss_pinn_ic : 115.3370 97.53691101074219 67897.0703125
Epoch [600/15000], Loss: 88.6734, Loss_data: 47.5881, Loss_dt: 40775.3359, Loss_pinn: 2029.8389 , Loss_pinn_ic : 107.0053 96.40901947021484 67410.8203125
Epoch [650/15000], Loss: 81.3549, Loss_data: 43.6766, Loss_dt: 37403.2148, Loss_pinn: 1804.8708 , Loss_pinn_ic : 94.6146 96.4136962890625 67609.25
Epoch [700/15000], Loss: 74.6544, Loss_data: 40.2085, Loss_dt: 34139.8086, Loss_pinn: 2276.9504 , Loss_pinn_ic : 78.3735 96.02378845214844 66637.5078125
Epoch [750/15000], Loss: 69.2417, Loss_data: 37.5263, Loss_dt: 31348.7559, Loss_pinn: 2951.7468 , Loss_pinn_ic : 71.4049 96.8886489868164 66639.3984375
Epoch [800/15000], Loss: 64.1624, Loss_data: 34.8207, Loss_dt: 28915.5293, Loss_pinn: 3627.1501 , Loss_pinn_ic : 63.4947 97.0738525390625 66050.8828125
Epoch [850/15000], Loss: 59.8015, Loss_data: 32.6792, Loss_dt: 26655.7051, Loss_pinn: 3995.4822 , Loss_pinn_ic : 67.0595 99.89994049072266 68096.96875
Epoch [900/15000], Loss: 55.7136, Loss_data: 30.4851, Loss_dt: 24757.2090, Loss_pinn: 4034.3098 , Loss_pinn_ic : 67.9248 98.93928527832031 65874.78125
Epoch [950/15000], Loss: 51.9023, Loss_data: 28.3680, Loss_dt: 23054.4414, Loss_pinn: 4154.6274 , Loss_pinn_ic : 64.4831 98.93769836425781 64652.125
Epoch [1000/15000], Loss: 48.9158, Loss_data: 26.6974, Loss_dt: 21739.6035, Loss_pinn: 4181.9712 , Loss_pinn_ic : 60.6079 99.70738220214844 64672.46875
Epoch [1050/15000], Loss: 46.2711, Loss_data: 25.2639, Loss_dt: 20513.1133, Loss_pinn: 4353.4272 , Loss_pinn_ic : 58.6649 101.12057495117188 65007.67578125
Epoch [1100/15000], Loss: 44.2049, Loss_data: 24.1512, Loss_dt: 19551.8496, Loss_pinn: 4455.2368 , Loss_pinn_ic : 56.2804 102.57921600341797 65303.56640625
Epoch [1150/15000], Loss: 42.6189, Loss_data: 23.2165, Loss_dt: 18900.9805, Loss_pinn: 4493.5469 , Loss_pinn_ic : 52.1136 103.43878173828125 65059.62109375
Epoch [1200/15000], Loss: 40.2953, Loss_data: 22.0635, Loss_dt: 17728.1992, Loss_pinn: 4503.5298 , Loss_pinn_ic : 53.2847 105.20860290527344 66070.125
Epoch [1250/15000], Loss: 38.6166, Loss_data: 21.1932, Loss_dt: 16912.0625, Loss_pinn: 4589.9448 , Loss_pinn_ic : 52.3574 106.3677749633789 66595.1328125
Epoch [1300/15000], Loss: 37.3776, Loss_data: 20.5827, Loss_dt: 16275.4609, Loss_pinn: 4677.0708 , Loss_pinn_ic : 51.7365 107.96134948730469 67293.6328125
Epoch [1350/15000], Loss: 36.8402, Loss_data: 20.1301, Loss_dt: 16188.7900, Loss_pinn: 4693.2480 , Loss_pinn_ic : 51.9448 107.09864807128906 67137.1484375
Epoch [1400/15000], Loss: 34.1860, Loss_data: 18.9666, Loss_dt: 14691.9043, Loss_pinn: 4775.8452 , Loss_pinn_ic : 49.9576 109.2451400756836 68014.8203125
Epoch [1450/15000], Loss: 32.8195, Loss_data: 18.2568, Loss_dt: 14033.7930, Loss_pinn: 4806.7988 , Loss_pinn_ic : 48.1652 110.09733581542969 68387.515625
Epoch [1500/15000], Loss: 33.1653, Loss_data: 18.3795, Loss_dt: 14255.6611, Loss_pinn: 4797.6938 , Loss_pinn_ic : 50.3794 113.4135971069336 69949.9765625
Epoch [1550/15000], Loss: 30.3259, Loss_data: 17.0130, Loss_dt: 12794.9531, Loss_pinn: 4745.2729 , Loss_pinn_ic : 43.4205 112.02420043945312 69422.828125
Epoch [1600/15000], Loss: 29.0814, Loss_data: 16.4041, Loss_dt: 12166.3662, Loss_pinn: 4700.0557 , Loss_pinn_ic : 40.8918 114.49292755126953 70713.640625
Epoch [1650/15000], Loss: 28.1208, Loss_data: 15.9432, Loss_dt: 11669.9189, Loss_pinn: 4682.3853 , Loss_pinn_ic : 39.4539 115.57910919189453 71549.7265625
Epoch [1700/15000], Loss: 27.4768, Loss_data: 15.7581, Loss_dt: 11210.9385, Loss_pinn: 4662.6118 , Loss_pinn_ic : 41.4681 117.63798522949219 73270.4296875
Epoch [1750/15000], Loss: 26.4528, Loss_data: 15.1824, Loss_dt: 10766.6885, Loss_pinn: 4662.0283 , Loss_pinn_ic : 37.5336 119.93450927734375 74130.5546875
Epoch [1800/15000], Loss: 27.1957, Loss_data: 15.5418, Loss_dt: 11145.4453, Loss_pinn: 4704.9136 , Loss_pinn_ic : 37.9122 120.69828796386719 74577.296875
Epoch [1850/15000], Loss: 24.9848, Loss_data: 14.5694, Loss_dt: 9912.1943, Loss_pinn: 4655.4453 , Loss_pinn_ic : 37.6983 124.9028091430664 77495.90625
Epoch [1900/15000], Loss: 24.1036, Loss_data: 14.1568, Loss_dt: 9438.3105, Loss_pinn: 4723.6895 , Loss_pinn_ic : 36.0962 128.18829345703125 79094.1171875
Epoch [1950/15000], Loss: 23.3021, Loss_data: 13.7690, Loss_dt: 9022.1533, Loss_pinn: 4767.3970 , Loss_pinn_ic : 34.2902 131.0240020751953 80568.4375
Epoch [2000/15000], Loss: 22.5145, Loss_data: 13.4209, Loss_dt: 8591.9199, Loss_pinn: 4688.5610 , Loss_pinn_ic : 32.8215 134.4921875 82971.9140625
Epoch [2050/15000], Loss: 21.6322, Loss_data: 13.0038, Loss_dt: 8131.7808, Loss_pinn: 4656.3057 , Loss_pinn_ic : 30.9478 137.99000549316406 85316.0390625
Epoch [2100/15000], Loss: 20.9896, Loss_data: 12.7806, Loss_dt: 7705.0288, Loss_pinn: 4746.3169 , Loss_pinn_ic : 29.2924 140.32778930664062 86864.8828125
Epoch [2150/15000], Loss: 20.2202, Loss_data: 12.4875, Loss_dt: 7223.1855, Loss_pinn: 4822.1475 , Loss_pinn_ic : 27.3556 143.8571014404297 89391.8984375
Epoch [2200/15000], Loss: 20.1838, Loss_data: 12.5111, Loss_dt: 7158.9019, Loss_pinn: 4870.7134 , Loss_pinn_ic : 26.7060 144.9403533935547 89870.3125
Epoch [2250/15000], Loss: 18.8814, Loss_data: 12.0174, Loss_dt: 6351.6235, Loss_pinn: 4882.7876 , Loss_pinn_ic : 24.0511 149.0771026611328 92420.6875
Epoch [2300/15000], Loss: 18.3463, Loss_data: 11.8297, Loss_dt: 6008.3823, Loss_pinn: 4858.5596 , Loss_pinn_ic : 22.3481 152.2225341796875 94249.375
Epoch [2350/15000], Loss: 18.3019, Loss_data: 11.8058, Loss_dt: 5989.9624, Loss_pinn: 4841.2632 , Loss_pinn_ic : 21.9409 156.1863555908203 96704.734375
Epoch [2400/15000], Loss: 17.7189, Loss_data: 11.5986, Loss_dt: 5616.5596, Loss_pinn: 4829.6055 , Loss_pinn_ic : 20.8263 158.5587921142578 98018.7890625
Epoch [2450/15000], Loss: 16.7160, Loss_data: 11.2226, Loss_dt: 4989.7104, Loss_pinn: 4844.9409 , Loss_pinn_ic : 19.2000 159.4226837158203 98002.0390625
Epoch [2500/15000], Loss: 16.2133, Loss_data: 11.0422, Loss_dt: 4672.0430, Loss_pinn: 4806.1753 , Loss_pinn_ic : 18.4694 160.91616821289062 99075.4765625
Epoch [2550/15000], Loss: 15.7160, Loss_data: 10.8490, Loss_dt: 4373.7720, Loss_pinn: 4749.7305 , Loss_pinn_ic : 18.2257 163.21524047851562 100559.0625
Epoch [2600/15000], Loss: 15.2619, Loss_data: 10.6977, Loss_dt: 4076.9773, Loss_pinn: 4693.9668 , Loss_pinn_ic : 17.8096 163.71978759765625 101017.6796875
Epoch [2650/15000], Loss: 14.9163, Loss_data: 10.5900, Loss_dt: 3837.1975, Loss_pinn: 4716.6919 , Loss_pinn_ic : 17.3841 163.252197265625 100500.03125
Epoch [2700/15000], Loss: 14.2501, Loss_data: 10.3271, Loss_dt: 3420.0925, Loss_pinn: 4858.4019 , Loss_pinn_ic : 17.0769 164.5801239013672 102004.625
Epoch [2750/15000], Loss: 13.8113, Loss_data: 10.1750, Loss_dt: 3140.5547, Loss_pinn: 4789.8809 , Loss_pinn_ic : 16.7228 165.5305938720703 103080.9921875
Epoch [2800/15000], Loss: 13.4142, Loss_data: 10.0498, Loss_dt: 2881.9978, Loss_pinn: 4664.9463 , Loss_pinn_ic : 15.9290 166.13186645507812 103648.0703125
Epoch [2850/15000], Loss: 13.1763, Loss_data: 9.9708, Loss_dt: 2733.3828, Loss_pinn: 4564.9946 , Loss_pinn_ic : 15.5986 167.07931518554688 104791.796875
Epoch [2900/15000], Loss: 12.7415, Loss_data: 9.8231, Loss_dt: 2452.1833, Loss_pinn: 4515.9111 , Loss_pinn_ic : 14.6838 167.383544921875 105060.2109375
Epoch [2950/15000], Loss: 12.9556, Loss_data: 9.9132, Loss_dt: 2585.4292, Loss_pinn: 4424.2686 , Loss_pinn_ic : 14.5047 167.49436950683594 104918.1796875
Epoch [3000/15000], Loss: 12.2866, Loss_data: 9.6800, Loss_dt: 2131.0959, Loss_pinn: 4616.1265 , Loss_pinn_ic : 13.8813 168.76669311523438 106342.8984375
Epoch [3050/15000], Loss: 12.1420, Loss_data: 9.6474, Loss_dt: 2058.4644, Loss_pinn: 4220.2114 , Loss_pinn_ic : 14.1687 170.31552124023438 107845.65625
Epoch [3100/15000], Loss: 11.8110, Loss_data: 9.5399, Loss_dt: 1840.8455, Loss_pinn: 4169.2202 , Loss_pinn_ic : 13.3601 169.84364318847656 107658.890625
Early stopping
Validation loss decreased (142.703156 --> 169.307953).  Saving model ...
Model( and tf values) saved: model/data_dt_pinn_ic/GFL_2nd_orderDynamicNN_1_3141_80000_100000_10000_None_None_1_0.001_0.0001_0.001_Static.pth
Total test trajectories 10
Loss: 133.78507996
MAE Loss: 4.29908180
Total trainable parameters 17026
