wandb:
  api_key: "c26f0418182418f6712b79b4457de4faa81b7524"
  project: "Project_PINNs_for_synchronous_machine"

time : 1 # Total time to be considered
num_of_points : 1000 # To be used when defining new collocation points, num per second, e.g. 1000*1s = 5K points
seed : 37 # Seed for reproducibility
modelling_method : True # For GFL: True, since in p.u.

model:
  model_flag : "GFL_2nd_order" # Choose between "SM_IB" or "SM4" or "SM_AVR" or "SM_AVR_GOV" or "SM6"
  model_num : 1 # Set of machine parameters to be used
  init_condition_bounds : 1 # Final number of set of initial conditions for the collocation points to be used
  sampling: "Lhs" # Choose between "Lhs" or "Linear" or "Random" for sampling the initial conditions for collocation points
  torch : True # Keep it True when training the model! and also using to create the collocaiton points

dataset:
  number : 1 # Number of dataset to be used
  shuffle: True # Shuffle the trajectories of the dataset # If True, the dataset will be shuffled
  split_ratio: 0.8 # Train and validation/test split ratio, the rest is splitted equally for validation/test
  validation_flag : True # Use validation set
  transform_input : "None" # Choose between "Std" or "MinMax" or "None" for input transformation
  transform_output : "None" # Choose between "Std" or "MinMax" or "None" for output transformation
  new_coll_points_flag : True # Use new collocation points or use from the dataset
  perc_of_data_points : 1 # Percentage of ground truth data points to be used for PINN training
  perc_of_col_points : 1 # Percentage of available collaction points to be used for PINN training

network:
  num_of_skip_data_points: 1   #23,19,1
  num_of_skip_col_points : 1
  num_of_skip_val_points : 1
  type : "DynamicNN" # Choose between "StaticNN" or "DynamicNN" or "PinnA" or "PinnB" or "PinnAA" this is the adj target one
  hidden_layers: 4
  hidden_dim: 64 # 20 if Kan
  loss_criterion : "MSELoss" # Choose between "MSELoss" or "L1Loss" or "SmoothL1Loss"
  time_factored_loss : False # Use time factored loss or not
  optimizer : "Adam" # Choose between "Adam" or "SGD" or "RMSprop" or "LBFGS"
  weight_init: "xavier_normal" # Choose between "xavier_normal" or "xavier_uniform" or "kaiming_normal" or "kaiming_uniform" or "normal"
  lr: 0.001
  lr_scheduler : "ExponentialLR" # Choose between "StepLR", "MultiStepLR", "ExponentialLR", "ReduceLROnPlateau", "No_scheduler"
  num_epochs: 25000
  batch_size: "None" # Choose between "None" or any integer value
  early_stopping: True
  early_stopping_patience: 2500
  early_stopping_min_delta: 1e-6
  weighting:
    flag_mean_weights : True # Create individual weights for each residual term of loss dt and loss pinn or not
    update_weight_method : "Static" # Choose between "Static", "ReLoBRaLo", "Dynamic", "Sam", "Gradient", "Ntk"
    update_weights_freq : 50 # if Dynamic or Sam, value must be >=1
    weights: [1, 1e-3, 1e-4, 1e-3] # [1, 1e-3, 1e-4, 1e-3] [0, 0, 1e-4, 1e-3]

#  weighting:
#    flag_mean_weights: True # Create individual weights for each residual term of loss dt and loss pinn or not
#    update_weight_method: "Static" # Choose between "Static", "ReLoBRaLo", "Dynamic", "Sam", "Gradient", "Ntk"
#    update_weights_freq: 50 # if Dynamic or Sam, value must be >=1
#    weights: 1

dirs:
  params_dir :  src/conf/params #${hydra:runtime.cwd}/src/conf/params
  init_conditions_dir : src/conf/initial_conditions #${hydra:runtime.cwd}/src/conf/initial_conditions
  dataset_dir : data #${hydra:runtime.cwd}/data
  model_dir : model #${hydra:runtime.cwd}/model