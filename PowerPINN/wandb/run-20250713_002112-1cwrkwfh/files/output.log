GFL_2nd_order data
Loading data from:  ./data/GFL_2nd_order/dataset_v1.pkl
/Users/nbhsbgnb/PycharmProjects/PythonProject/PowerPINN/src/nn/nn_dataset.py:178: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)
  training_sample = torch.tensor(training_sample, dtype=torch.float32) # convert the trajectory to tensor
Number of training samples:  720000 Number of validation samples:  90000 Number of testing samples:  90000
Number of different initial conditions for collocation points:  100
['delta', 'omega'] Variables
[[-3.14159, 3.14159], [-60, 60]] Set of values for init conditions
[10, 10] Iterations per value
Shape: (100, 2)
Selected deep learning model:  DynamicNN
Number of labeled training data: 720000 Number of collocation points: 100000 Number of collocation points (IC): 100 Number of validation data: 90000
Weights initialized as:  [1, 0, 0, 0]  are updated with scheme:  Static
getting in training
type:  validation, total_traj: 90, max_traj 20
Epoch [50/25000], Loss: 111.2984, Loss_data: 111.2984, Loss_dt: 66214.7266, Loss_pinn: 271527.3125 , Loss_pinn_ic : 519.8297 112.53328704833984 68296.2578125
type:  validation, total_traj: 90, max_traj 20
Epoch [100/25000], Loss: 106.4084, Loss_data: 106.4084, Loss_dt: 66199.0391, Loss_pinn: 142775.2344 , Loss_pinn_ic : 515.2842 107.5067367553711 68276.96875
type:  validation, total_traj: 90, max_traj 20
Epoch [150/25000], Loss: 102.4039, Loss_data: 102.4039, Loss_dt: 66091.5859, Loss_pinn: 137589.4531 , Loss_pinn_ic : 477.9110 103.41525268554688 68138.3671875
type:  validation, total_traj: 90, max_traj 20
Epoch [200/25000], Loss: 93.9876, Loss_data: 93.9876, Loss_dt: 65347.5664, Loss_pinn: 147808.1250 , Loss_pinn_ic : 370.8358 95.22804260253906 67245.5078125
type:  validation, total_traj: 90, max_traj 20
Epoch [250/25000], Loss: 78.4144, Loss_data: 78.4144, Loss_dt: 60746.6602, Loss_pinn: 131776.8906 , Loss_pinn_ic : 245.9370 79.95977020263672 62318.0
type:  validation, total_traj: 90, max_traj 20
Epoch [300/25000], Loss: 64.7113, Loss_data: 64.7113, Loss_dt: 54770.9141, Loss_pinn: 140212.3125 , Loss_pinn_ic : 179.0631 66.17362976074219 56159.9453125
type:  validation, total_traj: 90, max_traj 20
Epoch [350/25000], Loss: 55.5124, Loss_data: 55.5124, Loss_dt: 50957.3711, Loss_pinn: 142069.5938 , Loss_pinn_ic : 153.4093 56.6567497253418 52245.5234375
type:  validation, total_traj: 90, max_traj 20
Epoch [400/25000], Loss: 47.3607, Loss_data: 47.3607, Loss_dt: 47510.7227, Loss_pinn: 135396.1094 , Loss_pinn_ic : 118.9798 48.27899932861328 48955.26171875
type:  validation, total_traj: 90, max_traj 20
Epoch [450/25000], Loss: 41.0630, Loss_data: 41.0630, Loss_dt: 44219.9219, Loss_pinn: 132107.9219 , Loss_pinn_ic : 100.7560 42.71812057495117 46401.18359375
type:  validation, total_traj: 90, max_traj 20
Epoch [500/25000], Loss: 36.2792, Loss_data: 36.2792, Loss_dt: 40972.7852, Loss_pinn: 129436.5781 , Loss_pinn_ic : 81.9352 37.2688102722168 42711.42578125
type:  validation, total_traj: 90, max_traj 20
Epoch [550/25000], Loss: 32.3368, Loss_data: 32.3368, Loss_dt: 37997.3398, Loss_pinn: 135217.3125 , Loss_pinn_ic : 66.1599 32.854251861572266 39512.6796875
type:  validation, total_traj: 90, max_traj 20
Epoch [600/25000], Loss: 29.0336, Loss_data: 29.0336, Loss_dt: 35599.0508, Loss_pinn: 143687.9688 , Loss_pinn_ic : 57.1927 30.014461517333984 37538.765625
type:  validation, total_traj: 90, max_traj 20
Epoch [650/25000], Loss: 26.1406, Loss_data: 26.1406, Loss_dt: 33656.0039, Loss_pinn: 140284.8438 , Loss_pinn_ic : 48.1347 26.808286666870117 35145.3515625
type:  validation, total_traj: 90, max_traj 20
Epoch [700/25000], Loss: 24.6211, Loss_data: 24.6211, Loss_dt: 32391.7969, Loss_pinn: 135620.3125 , Loss_pinn_ic : 45.2060 25.704736709594727 34051.75390625
type:  validation, total_traj: 90, max_traj 20
Epoch [750/25000], Loss: 22.3445, Loss_data: 22.3445, Loss_dt: 30900.1621, Loss_pinn: 141571.5469 , Loss_pinn_ic : 39.7707 23.915363311767578 32474.6640625
